#! /usr/bin/env python

from CHIRPP_utils import my_cmd
import os.path


def write_script(fname, lines, force_overwrite=False):
    print(f"Writing {fname}.\n")
    if os.path.isfile(fname) and not force_overwrite:
        while True:
            user_input = input(f"{fname} already exists! Overwrite? [y/n]")
            if user_input == "y" or user_input == "Y":
                script = open(fname, "w")
                for line in lines:
                    script.write(f"{line}\n")
                script.write(f" \n")
                script.close()
                my_cmd(f"chmod +x {fname}", f"Make {fname} an executable.")
                return
            elif user_input == "n" or user_input == "N":
                while True:
                    user_input = input(f"Continue with {fname} as is? [y/n]")
                    if user_input == "y" or user_input == "Y":
                        return
                    elif user_input == "n" or user_input == "N":
                        exit(0)


def write_allParamCheck(force_overwrite=False):
    lines_allParamCheck = [
        "#!/bin/bash",
        "",
        "#SBATCH --ntasks=1             # Single Job is created, sets the # of jobs/commands launched",
        "#SBATCH --error=%x-%j.err      # Error file name = jobname-jobID.err",
        "",
        "# File to check all archive parameters and standardize them",
        "",
        "# Print job ID",
        'echo "Job ID: $SLURM_JOB_ID"',
        "",
        "###########################################################",
        "##                     Params to input                   ##",
        "###########################################################",
        "",
        "# Define the extension of the files to check",
        'ext=".ar"',
        "",
        "###########################################################",
        "##                      General set-up                   ##",
        "###########################################################",
        "",
        "# File to store filenames and parameter values",
        'paramList="paramList.txt"',
        "",
        "# Create a file of filename followed by param values",
        'echo "# File of parameter values - columns = filename,nbin,nchan,freq,bw,npol,dm,nsub,length" > "$paramList"',
        'echo "" >> "$paramList"',
        "",
        "# Use psrstat to pull parameters from the file",
        'vap -nc nbin,nchan,freq,bw,npol,dm,nsub,length *${ext} >> "$paramList" # by-file parameters',
        "",
        "# Sort the parameter list from most recent to oldest",
        'echo "# File of parameter values - columns = filename,nbin,nchan,freq,bw,npol,dm,nsub,length" > sorted_"$paramList"',
        'echo "# Copy of $paramList but reordered to be in descending order of data e.g. most recent is first." >> sorted_"$paramList"',
        'echo "" >> sorted_"$paramList"',
        'sort -t "_" -k5,5nr "$paramList" >> sorted_"$paramList"',
        "",
        "# Map the file lines to arrays",
        'mapfile -t lines < sorted_"$paramList"',
        "",
        "# Make an array for each column/parameter values, and arguments used in subint check later",
        "declare -a filename nbin nchan freq bw npol dm nsub length process_args",
        "",
        "# Loop through each line, extract parameters, and store in arrays",
        'for line in "${lines[@]}"; do',
        "    # Skip the header and empty lines",
        '    if [[ "$line" =~ ^#.* ]] || [[ -z "$line" ]]; then',
        "        continue",
        "    fi",
        "",
        "    # Split line into fields based on whitespace",
        '    read -r file_val nbin_val nchan_val freq_val bw_val npol_val dm_val nsub_val len_val<<< "$line"',
        "",
        "    # Populate respective arrays",
        '    filename+=("$file_val")',
        '    nbin+=("$nbin_val")',
        '    nchan+=("$nchan_val")',
        '    freq+=("$freq_val")',
        '    bw+=("$bw_val")',
        '    npol+=("$npol_val")',
        '    dm+=("$dm_val")',
        '    process_args+=("$file_val $nsub_val $len_val")',
        "done",
        "",
        "# Create folders to store files that fail each parameter's check",
        "mkdir -p nbinFail nchanFail freqFail bwFail npolFail dmFail",
        "",
        "###########################################################",
        "##                 subint duration check                 ##",
        "###########################################################",
        "",
        "# Check that each file has subintegrations of uniform length, within specified threshold",
        "# If not, remove first and last subint",
        "",
        "# Define the function to check subint duration for each file",
        "process_file() {",
        "    file=$(echo $1 | awk '{print $1}')",
        "    nsub=$(echo $1 | awk '{print $2}')",
        "    length=$(echo $1 | awk '{print $3}')",
        "",
        "    # Final subint index (after first subint is removed)",
        "    endsub=$((nsub-2))",
        "",
        "    # Define the length of each subint, should always be 10 sec",
        "    nsub_duration=10.0 # sec",
        "",
        "    # Define the threshold, arbitrary",
        "    threshold=0.01",
        "",
        "    # Calculate length % 10 sec",
        "    # If there is a remainder, one of the subints is non-uniform",
        "    # The first/last subints will be shorter due to backend start-up/shutdown",
        '    mod=$(python -c "printf {0} {1}")'.format(
            "'{float($length) %", "float($nsub_duration):.4f}'"
        ),
        "",
        "    # Check if mod > threshold",
        '    if python -c "import sys; sys.exit(0 if float($mod) > float($threshold) else 1)"; then',
        '        echo "File: $file meets the condition (length=$length % 10 sec = $mod > $threshold), removing first and last subintegration."',
        "",
        "        # Remove first and last subintegration",
        '        echo "delete subint 0"',
        'delete subint $endsub" > rmsubints_${file}.psrsh',
        "",
        '        psrsh rmsubints_${file}.psrsh -m "$file"',
        "",
        "        rm rmsubints_${file}.psrsh",
        "",
        "    else",
        '        echo "File: $file does not meet the condition (length=$length % 10 sec = $mod <= $threshold), no subintegrations removed."',
        "    fi",
        "",
        '    echo "---------------------------------------------"',
        "}",
        "",
        "export -f process_file",
        "",
        "# Use parallel to process files concurrently (limit ea. batch to $SLURM_CPUS_PER_TASK jobs at a time)",
        "printf '%s\\n' {0}".format(
            '"${process_args[@]}" | parallel --env _ -j $SLURM_CPUS_PER_TASK process_file'
        ),
        "",
        'echo "SUBINT Check complete..."',
        'echo "---------------------------------------------"',
        'echo "---------------------------------------------"',
        "",
        "###########################################################",
        "##                      NBIN check                       ##",
        "###########################################################",
        "",
        "# Logic: Whatever nbin value is most recent is presumably vetted and decreed to be 'best' for that pulsar (Ingrid Stairs)",
        "# Templates will be made from the highest resolution data (e.g. greatest nbin values).",
        "# A higher resolution template can be applied to lower resolution data as pat will automatically downsample. The reverse is not true.",
        "# NOTE: older data should always be lower-res (Emmanuel Fonseca)",
        "",
        "# Store for use in pam --setnbin in template creation - give priority to recent file parameters",
        'template_nbin="${nbin[0]}"',
        "",
        "# Initialize high_res",
        'high_res="$template_nbin"',
        "",
        'for val in "${nbin[@]}"; do',
        "    # Skip non-numeric values",
        '    if ! [[ "$val" =~ ^-?[0-9]+(\\.[0-9]+)?$ ]]; then',
        "        continue",
        "    fi",
        "",
        "    # Find max e.g. highest-resolution nbin",
        '    if (( $(echo "$val > $high_res" | bc -l) )); then',
        '        high_res="$val"',
        "    fi",
        "done",
        "",
        "# Check if high_res is equal to template_nbin",
        'if [ "$high_res" = "$template_nbin" ]; then',
        '    echo "Recent data is the highest resolution data (nbin=$high_res)"',
        '    echo "---------------------------------------------"',
        "else",
        '    echo "Recent data is NOT the highest resolution (high resolution nbin=$high_res, most recent nbin=$template_nbin)"',
        '    echo "Investigate this. The most recent data in CHIME should be the highest/optimal resolution for that pulsar."',
        '    echo "---------------------------------------------"',
        "fi",
        "",
        "# Print results",
        'echo "The following will be input into config.sh:"',
        'echo "template_nbin=${nbin[0]} # Most recent nbin value"',
        'echo "NBIN Check complete..."',
        'echo "---------------------------------------------"',
        'echo "---------------------------------------------"',
        "",
        "###########################################################",
        "##               All other params check                  ##",
        "###########################################################",
        "",
        "# For all other parameters we will find the most common parameter value",
        "#    and reject any file that does not have that value for the given parameter",
        "#",
        "# Temporary log files are created for each parameter recording the filename and value that failed the check",
        "#    all files that failed at least one check is moved into the common_failures/ folder which contains a folder",
        "#    for each parameter and the log detailing each file that failed",
        "# The file: unique_failures.txt records the filename only once from each check",
        "#    e.g. if a file failed freq and dm checks the filename and value will be recorded in freqFail/freqFail.log and dmFail/dmFail.log",
        "#    but the filename will only be recorded once in unique_failures.txt and the file will be in common_failures/",
        "",
        "# Function to find the most common value in an array",
        "find_most_common() {",
        '    local array=("$@")',
        "    printf {0} {1} | sort | uniq -c | sort -nr | head -n1 | awk {2}".format(
            "'%s\\n'", '"${array[@]}"', "'{print $2}'"
        ),
        "}",
        "",
        "# Get most common DM value",
        "common_dm=$(find_most_common $dm)",
        'echo "common_dm=${common_dm} # Most common DM value"',
        "",
        "# Function to check array against the most common value and log mismatches",
        "check_array() {",
        "    local -n array=$1",
        "    local array_name=$2",
        '    local common_value=$(find_most_common "${array[@]}")',
        "",
        '    echo "Most common value in $array_name: $common_value"',
        "",
        '    local log_file="${array_name}Fail/${array_name}Fail.log"',
        '    touch "$log_file"',
        "",
        "    # Initialize flag to check if we have written the header",
        "    local header_written=false",
        "",
        '    for i in "${!array[@]}"; do',
        '        if [ "${array[$i]}" != "$common_value" ]; then',
        '            if [ "$header_written" = false ]; then',
        '                echo "# Filenames and parameter values for files that are not the most common value ($common_value) for parameter: $array_name" >> "$log_file"',
        "                header_written=true",
        "            fi",
        '            echo "${filename[$i]}    ${array[$i]}" >> "$log_file"',
        "        fi",
        "    done",
        "",
        "    # If no mismatches were found, log that all parameter values are uniform",
        '    if [ "$header_written" = false ]; then',
        '        echo "# All parameter values are uniform for $array_name = $common_value" >> "$log_file"',
        "    fi",
        "}",
        "",
        "# Check all arrays against their most common values",
        'check_array nchan "nchan"',
        'check_array freq "freq"',
        'check_array bw "bw"',
        'check_array npol "npol"',
        'echo ""',
        "",
        "# Create a new file that lists a filename only once from all *Fail.log files",
        "declare -a all_files",
        "for fail_log in *Fail/*.log; do",
        "    # Read the log file line by line",
        "    while IFS= read -r line; do",
        "        # Skip lines that are comments or empty",
        '        if [[ "$line" =~ ^#.* ]] || [[ -z "$line" ]]; then',
        "            continue",
        "        fi",
        "        # Add the filename (first word) to the all_files array",
        '        file=$(echo "$line" | awk ' + "'{print $1}')",
        '        all_files+=("$file")',
        '    done < "$fail_log"',
        "done",
        "",
        "# Ensure all_files array is not empty before creating unique_failures.txt",
        'if [ "${#all_files[@]}" -gt 0 ]; then',
        '    echo "All files that failed a parameter check have been logged in unique_failures.txt"',
        "    printf '%s\\n' {0}".format(
            '"${all_files[@]}" | sort | uniq > unique_failures.txt'
        ),
        "else",
        '    echo "No files to list in unique_failures.txt - no files failed a parameter check."',
        "fi",
        "",
        "mkdir -p common_failures",
        "",
        "# Move all files listed in unique_failures.txt into common_failures",
        "while IFS= read -r file; do",
        '    if [ -f "$file" ]; then',
        '        mv "$file" common_failures/',
        "    else",
        '        echo "File not found: $file"',
        "    fi",
        "done < unique_failures.txt",
        'echo "All files that failed a parameter check have been moved to common_failures/"',
        "",
        "# Move all *Fail folders into common_failures",
        "for fail_dir in *Fail; do",
        '    if [ -d "$fail_dir" ]; then',
        '        mv "$fail_dir" common_failures/',
        "    else",
        '        echo "Directory not found: $fail_dir"',
        "    fi",
        "done",
        "mv sorted_paramList.txt common_failures/",
        "mv paramList.txt common_failures/",
        'echo "All PARAMETERFail, parameter logs, and PARAMETERFail.log{0}"'.format(
            "'s and files have been moved to common_failures/"
        ),
        "",
        'echo "All other parameter (nchan,freq,bw,npol,dm) check'
        + "'s are complete..."
        + '"',
        'echo "---------------------------------------------"',
        'echo "---------------------------------------------"',
        "",
        'echo "allParamCheck.sh has finished...  .      .        ."',
    ]
    write_script(
        "allParamCheck.sh", lines_allParamCheck, force_overwrite=force_overwrite
    )


def write_beamWeight(force_overwrite=False):
    lines_beamWeight = [
        "#!/bin/bash",
        "",
        "#SBATCH --error=%x-%j.err                 # Error file name = jobname-jobID.err",
        "",
        "# Print job ID",
        'echo "Job ID: $SLURM_JOB_ID"',
        "",
        "source config.sh",
        "",
        "# Check if there is more than one pulsar data in the directory",
        'pulsar_name=$(check_pulsar_names "$data_directory")',
        "status=$?",
        "",
        "# Check the status and handle errors",
        "if [ $status -ne 0 ]; then",
        "    exit $status",
        "fi",
        "",
        "# If we reach here, it means exactly one pulsar name was found and stored in pulsar_name",
        'echo "Pulsar name found: $pulsar_name"',
        "",
        "# Run beam weighting on each file",
        "add_beam -vv -e bmwt CHIME*.clfd >>beamWeight_${pulsar_name}-${SLURM_JOB_ID}.out 2>>beamWeight_${pulsar_name}-${SLURM_JOB_ID}.err",
    ]
    write_script("beamWeight.sh", lines_beamWeight, force_overwrite=force_overwrite)


def write_chimezap(force_overwrite=False):
    lines_chimezap = [
        "#!/usr/bin/env psrsh",
        "",
        "# A PSRSH script to conduct RFI mitigation on CHIME/Pulsar fold-mode data.",
        "",
        "#########################",
        "# Manual excision steps #",
        "#########################",
        "",
        "# Prior to September 2020, the vast majority of RFI originated from either",
        "# the LTE cellular network, digital TV stations and a handful of narrow-band",
        "# emitters.",
        "zap chan 0 34-47 113-178 185-210 218-254 552-568 584-597 631-644 677-693 754-762 788-791 854-860 873-875 887",
        "",
        "# The local 5G network signal came online gradually over a few days, thus",
        "# only part of the nominal 5G band was initially corrupted. This affected",
        "# data between: August 30-31, 2020, i.e., 59091 <= MJD < 59093, chans=444-469",
        "if ($int[0]:mjd>=59091 && $int[0]:mjd<59093) zap chan 444-469",
        "",
        "# The 5G network signal came fully online on: September 1, 2020, i.e., MJD >= 59093, chans=405-469",
        "if ($int[0]:mjd>=59093) zap chan 405-469",
        "",
        "# A new RFI band (origin unknown) appeared on: November 5, 2021, i.e., MJD >= 59523, chans=83-107",
        "if ($int[0]:mjd>=59523) zap chan 83-107",
        "",
        "# Also include list of additional commonly-corrupted channels, from Emmanuel Fonseca",
        "zap chan 572 575 767 772 799 808 846 882 895",
        "",
        "# At this point, there may well be several channels or subintegrations",
        "# where a large fraction of data have been flagged. Extend the mask by",
        "# zapping channels where >75% of subintegrations are flagged AND by",
        "# zapping subintegrations where >75% of channels are flagged.",
        "zap extend tcutoff=0.75 fcutoff=0.75",
        "zap extend",
    ]
    write_script("chime_zap.psh", lines_chimezap, force_overwrite=force_overwrite)


def write_clean(force_overwrite=False):
    lines_clean = [
        "#!/bin/bash",
        "",
        "#SBATCH --error=%x-%j.err                 # Error file name = jobname-jobID.err",
        "",
        "# Print job ID",
        'echo "Job ID: $SLURM_JOB_ID"',
        "",
        "source config.sh",
        "",
        "# Check if there is more than one pulsar data in the directory",
        'pulsar_name=$(check_pulsar_names "$data_directory")',
        "status=$?",
        "",
        "# Check the status and handle errors",
        "if [ $status -ne 0 ]; then",
        "    exit $status",
        "fi",
        "",
        "# If we reach here, it means exactly one pulsar name was found and stored in pulsar_name",
        'echo "Pulsar name found: $pulsar_name"',
        "",
        "# Run clfd",
        "for f in $(ls CHIME*.zap); do",
        "    clfd $f >> clean_${pulsar_name}-${SLURM_JOB_ID}.out 2>>clean_${pulsar_name}-${SLURM_JOB_ID}.err",
        "done",
    ]
    write_script("clean.sh", lines_clean, force_overwrite=force_overwrite)


def write_clean5G(force_overwrite=False):
    lines_clean5G = [
        "#!/bin/bash",
        "",
        "#SBATCH --error=%x-%j.err                 # Error file name = jobname-jobID.err",
        "",
        "# Print job ID",
        'echo "Job ID: $SLURM_JOB_ID"',
        "",
        "source config.sh",
        "",
        "# Check if there is more than one pulsar data in the directory",
        'pulsar_name=$(check_pulsar_names "$data_directory")',
        "status=$?",
        "",
        "# Check the status and handle errors",
        "if [ $status -ne 0 ]; then",
        "    exit $status",
        "fi",
        "",
        "# If we reach here, it means exactly one pulsar name was found and stored in pulsar_name",
        'echo "Pulsar name found: $pulsar_name"',
        "",
        "# Zap known bad channels (5G zapping from Bradley plus list of commonly bad channels from Emmanuel)",
        "for f in $(ls CHIME*.ar); do ",
        "    psrsh chime_zap.psh -e ar.zap $f >> clean5G_${pulsar_name}-${SLURM_JOB_ID}.out 2>>clean5G_${pulsar_name}-${SLURM_JOB_ID}.err",
        "done",
    ]
    write_script("clean5G.sh", lines_clean5G, force_overwrite=force_overwrite)


def write_config(force_overwrite=False):
    lines_config = [
        "#!/bin/bash",
        "",
        "# Configuration file for CHIME/Pulsar data processing and TOA generation",
        "# Based on the pipeline created for the NANOGrav 20-year data set",
        "",
        "###########################################################",
        "##                     Paths to input                    ##",
        "###########################################################",
        "",
        "data_directory=$(pwd)",
        "",
        "# Directory containing the par files",
        'par_directory="${HOME}/projects/rrg-istairs-ad/DR3/NANOGrav_15y/par/tempo2"',
        "",
        "###########################################################",
        "##                  Ext/params to input                  ##",
        "###########################################################",
        "",
        "# This variable is output by allParamCheck.sh",
        "template_nbin=512 # Most recent nbin value",
        "",
        "# This variable is output by allParamCheck.sh",
        "dm=false # Most common DM value in file headers",
        '         # A value of "false" will skip the step of updating header DM values',
        '         # A value of "ephemeris" will use the DM value in the provided ephemeris',
        "# Maximum subint duration in seconds (lesser of 1 hour or 2.5% of orbital period, if any)",
        "# Note: the scrunch step is designed to produce subints of equal length.",
        "# So if files have multiple subints, their length will likely be shorter than this.",
        "max_subint=3600.0",
        "",
        "# Desired number of subbands",
        "nsubbands=64",
        "",
        "# What is the file extension to run template creation on?",
        'template_ext=".ftp" # ex: .zap or _trimmed.fits',
        "",
        "# Flags to use in each TOA saved in the *.tim file",
        'tim_flags="-f CHIME -be CHIME -fe Rcvr_CHIME"',
        "",
        "# Smoothed template filename (.sm)",
        'template="added.trimmed.sm"',
        "",
        "# Sanity check",
        'echo "Sanity check..."',
        'echo "---------------------------------------------"',
        'echo "Looking for pulsar data in directory: $data_directory"',
        'echo "Looking for par files in directory: $par_directory"',
        'echo "Template nbin: $template_nbin"',
        'echo "Maximum subint duration: $max_subint s"',
        'echo "Number of subbands: $nsubbands"',
        'echo "Template creation will use files with extension: $template_ext"',
        'echo "Your tim file will use these flags: $tim_flags"',
        'echo "---------------------------------------------"',
        'echo "---------------------------------------------"',
        'echo "Is this what you mean to do? If not edit config.sh (ctrl+C now)!"',
        'echo "---------------------------------------------"',
        'echo "---------------------------------------------"',
        "",
        "# Find all unique beam variations",
        "beam_variations=$(find {0} -maxdepth 1 -name 'CHIME*beam_[0-9]*.ar' | grep -oE 'beam_[0-9]+' | sort -u)".format(
            '"$data_directory"'
        ),
        'num_beam=$(echo "$beam_variations" | wc -l)',
        "",
        "###########################################################",
        "##                      Convience funcs                  ##",
        "###########################################################",
        "",
        "# Function to check if a file exists",
        "check_file_exists() {",
        '    local file_path="$data_directory/$1"',
        '    if [ -e "$file_path" ]; then',
        '        echo "File {0} was created."'.format("'$file_path'"),
        "    else",
        '        echo "Error: File {0} was not created."'.format("'$file_path'"),
        "    fi",
        "}",
        "",
        "# Function to remove a file, if it exists",
        "remove_file_if_exists() {",
        '    local file_path="$1"',
        '    if [ -e "$file_path" ]; then',
        '        echo "Removing {0}."'.format("'$file_path'"),
        "        rm $file_path",
        "    fi",
        "}",
        "",
        "# Function to check for multiple pulsar data in the directory",
        "# Error messages are redirected to stderr (e.g. >&2) so only the pulsar name/par path/parfile are stored in the output",
        "check_pulsar_names() {",
        '    local data_directory="$1"',
        "    local pulsar_names=()",
        "",
        "    # Loop through each file in the directory",
        '    for file in "$data_directory"/*; do',
        "        # Check if the file exists and is a regular file",
        '        if [ -f "$file" ]; then',
        "            # Find the pulsar name pattern in the file name",
        '            local pulsar_name=$(basename "$file" | grep -Eo {0})'.format(
            "'[JB][0-9]{4}[+-][0-9]{2,4}'"
        ),
        "",
        "            # If a pulsar name is found, add it to the array",
        '            if [ -n "$pulsar_name" ]; then',
        '                pulsar_names+=("$pulsar_name")',
        "            fi",
        "        fi",
        "    done",
        "",
        "    # Determine the result based on the number of unique pulsar names",
        '    local unique_pulsar_names=($(printf "%s\\n" "${pulsar_names[@]}" | sort -u))',
        "",
        '    if [ "${#unique_pulsar_names[@]}" -eq 0 ]; then',
        '        echo "Error: No pulsar names found." >&2',
        "        return 1",
        '    elif [ "${#unique_pulsar_names[@]}" -eq 1 ]; then',
        '        echo "${unique_pulsar_names[0]}"',
        "        return 0",
        "    else",
        '        echo "Error: Multiple pulsar data is in the directory." >&2',
        '        echo "Pulsar names found: $(printf {0} {1})" >&2'.format(
            '"%s\\n"', '"${unique_pulsar_names[@]}"'
        ),
        "        return 2",
        "    fi",
        "}",
        "",
        "# Function to find the par file",
        "find_par() {",
        '    local pulsar_name="$1"',
        "",
        '    if [ -z "$par_directory" ]; then',
        '        echo "Error: Par directory is not defined." >&2',
        "        return 1",
        "    fi",
        "",
        "    # Find the par file",
        '    local par_file=$(find "$par_directory" -type f -name "[JB]${pulsar_name:1}*.par" -print -quit)',
        "",
        '    if [ -n "$par_file" ]; then',
        '        echo "$par_file"',
        "        return 0",
        "    else",
        '        echo "Error: Par file not found in $par_directory" >&2',
        "        return 1",
        "    fi",
        "}",
    ]
    write_script("config.sh", lines_config, force_overwrite=force_overwrite)


def write_dateCheck(force_overwrite=False):
    lines_dateCheck = [
        "#!/bin/bash",
        "",
        "# Script to move files based on MJD cutoff (MJD<=58600)",
        "",
        "# Counter to track the number of moved files",
        "count=0",
        "",
        "# Create the dateFail directory if it doesn't exist",
        "mkdir -p dateFail",
        "",
        "# Loop through files and move based on MJD cutoff (MJD<=58600)",
        "for number in $(ls | grep -oE 'beam_[0-9]+_([0-9]{5})_' | grep -oE '[0-9]{5}' | awk '$1 <= 58600'); do",
        "    # Find and move files with matching MJD in filename to dateFail directory",
        '    find . -maxdepth 1 -type f -name "*beam_[0-9]_${0}_*" -exec mv {1} dateFail/ \\; -exec sh -c {2} sh {3} \\;'.format(
            "{number}", "{}", "'echo {0}'".format('"Moved: $1"'), "{}"
        ),
        "    ((count++)) # Increment count",
        "done",
        "",
        "# Print the total number of moved files",
        'echo "Total files moved: $count"',
    ]
    write_script("dateCheck.sh", lines_dateCheck, force_overwrite=force_overwrite)


def write_ephemNconvert(force_overwrite=False):
    lines_ephemNconvert = [
        "#!/bin/bash",
        "",
        "#SBATCH --error=%x-%j.err                 # Error file name = jobname-jobID.err",
        "",
        "# Print job ID",
        'echo "Job ID: $SLURM_JOB_ID"',
        "",
        "source config.sh",
        "",
        "# Check if there is more than one pulsar data in the directory",
        'pulsar_name=$(check_pulsar_names "$data_directory")',
        "status=$?",
        "",
        "# Check the status and handle errors",
        "if [ $status -ne 0 ]; then",
        "    exit $status",
        "fi",
        "",
        "# If we reach here, it means exactly one pulsar name was found and stored in pulsar_name",
        'echo "Pulsar name found: $pulsar_name"',
        "",
        "# Find par file to install ephemeris",
        'par_file=$(find_par "$pulsar_name")',
        "status=$?",
        "",
        "# Check the status and handle errors",
        "if [ $status -ne 0 ]; then",
        "    exit $status",
        "fi",
        "",
        "# If we reach here, it means the par file was found and stored in par_file",
        'echo "Par file found: $par_file"',
        'echo "---------------------------------------------"',
        "",
        "# Our par files use the tempo site code for CHIME, 'CH'. So we set:",
        "export TEMPO2_ALIAS='tempo'",
        "",
        "# Install ephemeris before averaging to ensure best data quality (Bradley Meyers)",
        "# And convert to a psrfits format for compatibility downstream",
        "# Also update header DMs, if desired",
        'if [ "$dm" = "ephemeris" ]; then',
        "    for f in $(ls CHIME*.ar); do",
        "        pam -p -E ${par_file} --update_dm -a PSRFITS -u . $f >> ephemNconvert_${pulsar_name}-${SLURM_JOB_ID}.out 2>>ephemNconvert_${pulsar_name}-${SLURM_JOB_ID}.err",
        "    done",
        'elif [ "$dm" = true ]; then',
        "    for f in $(ls CHIME*.ar); do",
        "        pam -p -E ${par_file} -d ${dm} -a PSRFITS -u . $f >> ephemNconvert_${pulsar_name}-${SLURM_JOB_ID}.out 2>>ephemNconvert_${pulsar_name}-${SLURM_JOB_ID}.err",
        "    done",
        "else",
        "    for f in $(ls CHIME*.ar); do",
        "        pam -p -E ${par_file} -a PSRFITS -u . $f >> ephemNconvert_${pulsar_name}-${SLURM_JOB_ID}.out 2>>ephemNconvert_${pulsar_name}-${SLURM_JOB_ID}.err",
        "    done",
        "fi",
    ]
    write_script(
        "ephemNconvert.sh", lines_ephemNconvert, force_overwrite=force_overwrite
    )


def write_newParamCheck(force_overwrite=False):
    lines_newParamCheck = [
        "#!/bin/bash",
        "",
        "#SBATCH --ntasks=1             # Single Job is created, sets the # of jobs/commands launched",
        "#SBATCH --error=%x-%j.err      # Error file name = jobname-jobID.err",
        "",
        "# File to check all archive parameters and standardize them",
        "",
        "# Print job ID",
        'echo "Job ID: $SLURM_JOB_ID"',
        "",
        "###########################################################",
        "##                     Params to input                   ##",
        "###########################################################",
        "",
        "# Define the extension of the files to check",
        'ext=".ar"',
        "",
        "###########################################################",
        "##                      General set-up                   ##",
        "###########################################################",
        "",
        "# File to store filenames and parameter values",
        'paramList="newparamList.txt"',
        'oldparamList="sorted_paramList.txt"',
        "",
        "# Create a file of filename followed by param values",
        'echo "# File of parameter values - columns = filename,nbin,nchan,freq,bw,npol,dm,nsub,length" > "$paramList"',
        'echo "" >> "$paramList"',
        "",
        "# Use psrstat to pull parameters from the file",
        'vap -nc nbin,nchan,freq,bw,npol,dm,nsub,length *${ext} >> "$paramList" # by-file parameters',
        "",
        "# Sort the parameter list from most recent to oldest",
        'echo "# File of parameter values - columns = filename,nbin,nchan,freq,bw,npol,dm,nsub,length" > sorted_"$paramList"',
        'echo "# Copy of $paramList but reordered to be in descending order of data e.g. most recent is first." >> sorted_"$paramList"',
        'echo "" >> sorted_"$paramList"',
        'sort -t {0} -k5,5nr "$paramList" >> sorted_"$paramList"'.format("'_'"),
        "",
        "# Map the file lines to arrays",
        'mapfile -t lines < sorted_"$paramList"',
        "mapfile -t old_lines < $oldparamList",
        "",
        "# Make an array for each column/parameter values, and arguments used in subint check later",
        "declare -a filename nbin nchan freq bw npol dm nsub length process_args",
        "declare -a old_filename old_nbin old_nchan old_freq old_bw old_npol old_dm old_nsub old_length old_process_args",
        "",
        "# Loop through each line, extract parameters, and store in arrays",
        'for line in "${lines[@]}"; do',
        "    # Skip the header and empty lines",
        '    if [[ "$line" =~ ^#.* ]] || [[ -z "$line" ]]; then',
        "        continue",
        "    fi",
        "",
        "    # Split line into fields based on whitespace",
        '    read -r file_val nbin_val nchan_val freq_val bw_val npol_val dm_val nsub_val len_val<<< "$line"',
        "",
        "    # Populate respective arrays",
        '    filename+=("$file_val")',
        '    nbin+=("$nbin_val")',
        '    nchan+=("$nchan_val")',
        '    freq+=("$freq_val")',
        '    bw+=("$bw_val")',
        '    npol+=("$npol_val")',
        '    dm+=("$dm_val")',
        '    process_args+=("$file_val $nsub_val $len_val")',
        "done",
        "",
        'for line in "${old_lines[@]}"; do',
        "    # Skip the header and empty lines",
        '    if [[ "$line" =~ ^#.* ]] || [[ -z "$line" ]]; then',
        "        continue",
        "    fi",
        "",
        "    # Split line into fields based on whitespace",
        '    read -r file_val nbin_val nchan_val freq_val bw_val npol_val dm_val nsub_val len_val<<< "$line"',
        "",
        "    # Populate respective arrays",
        '    old_filename+=("$file_val")',
        '    old_nbin+=("$nbin_val")',
        '    old_nchan+=("$nchan_val")',
        '    old_freq+=("$freq_val")',
        '    old_bw+=("$bw_val")',
        '    old_npol+=("$npol_val")',
        '    old_dm+=("$dm_val")',
        '    old_process_args+=("$file_val $nsub_val $len_val")',
        "done",
        "",
        "# Create folders to store files that fail each parameter's check",
        "mkdir -p nbinFail nchanFail freqFail bwFail npolFail dmFail",
        "",
        "###########################################################",
        "##                 subint duration check                 ##",
        "###########################################################",
        "",
        "# Check that each file has subintegrations of uniform length, within specified threshold",
        "# If not, remove first and last subint",
        "",
        "# Define the function to check subint duration for each file",
        "process_file() {",
        "    file=$(echo $1 | awk '{print $1}')",
        "    nsub=$(echo $1 | awk '{print $2}')",
        "    length=$(echo $1 | awk '{print $3}')",
        "",
        "    # Final subint index (after first subint is removed)",
        "    endsub=$((nsub-2))",
        "",
        "    # Define the length of each subint, should always be 10 sec",
        "    nsub_duration=10.0 # sec",
        "",
        "    # Define the threshold, arbitrary",
        "    threshold=0.01",
        "",
        "    # Calculate length % 10 sec",
        "    # If there is a remainder, one of the subints is non-uniform",
        "    # The first/last subints will be shorter due to backend start-up/shutdown",
        '    mod=$(python -c "printf({0}{1})")'.format(
            "'{float($length) %", " float($nsub_duration):.4f}'"
        ),
        "",
        "    # Check if mod > threshold",
        '    if python -c "import sys; sys.exit(0 if float($mod) > float($threshold) else 1)"; then',
        '        echo "File: $file meets the condition (length=$length % 10 sec = $mod > $threshold), removing first and last subintegration."',
        "",
        "        # Remove first and last subintegration",
        '        echo "delete subint 0"',
        'delete subint "$endsub" > rmsubints_${file}.psrsh',
        "",
        '        psrsh rmsubints_${file}.psrsh -m "$file"',
        "",
        "        rm rmsubints_${file}.psrsh",
        "",
        "    else",
        '        echo "File: $file does not meet the condition (length=$length % 10 sec = $mod <= $threshold), no subintegrations removed."',
        "    fi",
        "",
        '    echo "---------------------------------------------"',
        "}",
        "",
        "export -f process_file",
        "",
        "# Use parallel to process files concurrently (limit ea. batch to $SLURM_CPUS_PER_TASK jobs at a time)",
        "printf '%s\\n' {0} | parallel --env _ -j $SLURM_CPUS_PER_TASK process_file".format(
            '"${process_args[@]}"'
        ),
        "",
        'echo "SUBINT Check complete..."',
        'echo "---------------------------------------------"',
        'echo "---------------------------------------------"',
        "",
        "###########################################################",
        "##                      NBIN check                       ##",
        "###########################################################",
        "",
        '# Logic: Whatever nbin value is most recent is presumably vetted and decreed to be "best" for that pulsar (Ingrid Stairs)',
        "# Templates will be made from the highest resolution data (e.g. greatest nbin values).",
        "# A higher resolution template can be applied to lower resolution data as pat will automatically downsample. The reverse is not true.",
        "# NOTE: older data should always be lower-res (Emmanuel Fonseca)",
        "",
        "# Store for use in pam --setnbin in template creation - give priority to recent file parameters",
        'template_nbin="${old_nbin[0]}"',
        "",
        "# Initialize high_res",
        'high_res="$template_nbin"',
        "",
        'for val in "${nbin[@]}"; do',
        "    # Skip non-numeric values",
        '    if ! [[ "$val" =~ ^-?[0-9]+(\\.[0-9]+)?$ ]]; then',
        "        continue",
        "    fi",
        "",
        "    # Find max e.g. highest-resolution nbin",
        '    if (( $(echo "$val > $high_res" | bc -l) )); then',
        '        high_res="$val"',
        "    fi",
        "done",
        "",
        "# Check if high_res is equal to template_nbin",
        'if [ "$high_res" = "$template_nbin" ]; then',
        '    echo "Recent data is the highest resolution data (nbin=$high_res)"',
        '    echo "---------------------------------------------"',
        "else",
        '    echo "Recent data is NOT the highest resolution (high resolution nbin=$high_res, most recent nbin=$template_nbin)"',
        '    echo "Investigate this. The most recent data in CHIME should be the highest/optimal resolution for that pulsar."',
        '    echo "---------------------------------------------"',
        "fi",
        "",
        "# Print results",
        'echo "The following will be input into config.sh:"',
        'echo "template_nbin=${template_nbin} # Most recent nbin value"',
        'echo "NBIN Check complete..."',
        'echo "---------------------------------------------"',
        'echo "---------------------------------------------"',
        "",
        "###########################################################",
        "##               All other params check                  ##",
        "###########################################################",
        "",
        "# For all other parameters we will find the most common parameter value",
        "#    and reject any file that does not have that value for the given parameter",
        "#",
        "# Temporary log files are created for each parameter recording the filename and value that failed the check",
        "#    all files that failed at least one check is moved into the common_failures/ folder which contains a folder",
        "#    for each parameter and the log detailing each file that failed",
        "# The file: unique_failures.txt records the filename only once from each check",
        "#    e.g. if a file failed freq and dm checks the filename and value will be recorded in freqFail/freqFail.log and dmFail/dmFail.log",
        "#    but the filename will only be recorded once in unique_failures.txt and the file will be in common_failures/",
        "",
        "# Function to find the most common value in an array",
        "find_most_common() {",
        '    local array=("$@")',
        "    printf '%s\\n' {0} | sort | uniq -c | sort -nr | head -n1 | awk {1}".format(
            '"${array[@]}"', "'{print $2}'"
        ),
        "}",
        "",
        "# Get most common DM value",
        "common_dm=$(find_most_common $old_dm)",
        'echo "common_dm=${common_dm} # Most common DM value"',
        "",
        "# Function to check array against the most common value and log mismatches",
        "check_array() {",
        "    local -n array=$1",
        "    local -n old_array=$2",
        "    local array_name=$3",
        '    local common_value=$(find_most_common "${old_array[@]}")',
        "",
        '    echo "Most common $array_name value: $common_value"',
        "",
        '    local log_file="${array_name}Fail/${array_name}Fail.log"',
        '    touch "$log_file"',
        "",
        "    # Initialize flag to check if we have written the header",
        "    local header_written=false",
        "",
        '    for i in "${!array[@]}"; do',
        '        if [ "${array[$i]}" != "$common_value" ]; then',
        '            if [ "$header_written" = false ]; then',
        '                echo "# Filenames and parameter values for files that are not the most common value ($common_value) for parameter: $array_name" >> "$log_file"',
        "                header_written=true",
        "            fi",
        '            echo "${filename[$i]}    ${array[$i]}" >> "$log_file"',
        "        fi",
        "    done",
        "",
        "    # If no mismatches were found, log that all parameter values are uniform",
        '    if [ "$header_written" = false ]; then',
        '        echo "# All parameter values are uniform for $array_name = $common_value" >> "$log_file"',
        "    fi",
        "}",
        "",
        "# Check all arrays against their most common values",
        'check_array nchan old_nchan "nchan"',
        'check_array freq old_freq "freq"',
        'check_array bw old_bw "bw"',
        'check_array npol old_npol "npol"',
        'echo ""',
        "",
        "# Create a new file that lists a filename only once from all *Fail.log files",
        "declare -a all_files",
        "for fail_log in *Fail/*.log; do",
        "    # Read the log file line by line",
        "    while IFS= read -r line; do",
        "        # Skip lines that are comments or empty",
        '        if [[ "$line" =~ ^#.* ]] || [[ -z "$line" ]]; then',
        "            continue",
        "        fi",
        "        # Add the filename (first word) to the all_files array",
        '        file=$(echo "$line" | awk {0})'.format("'{print $1}'"),
        '        all_files+=("$file")',
        '    done < "$fail_log"',
        "done",
        "",
        "# Ensure all_files array is not empty before creating unique_failures.txt",
        'if [ "${#all_files[@]}" -gt 0 ]; then',
        '    echo "All files that failed a parameter check have been logged in unique_failures.txt"',
        "    printf {0} {1} | sort | uniq > unique_failures.txt".format(
            "'%s\\n'", '"${all_files[@]}"'
        ),
        "else",
        '    echo "No files to list in unique_failures.txt - no files failed a parameter check."',
        "fi",
        "",
        "mkdir -p common_failures",
        "",
        "# Move all files listed in unique_failures.txt into common_failures",
        "while IFS= read -r file; do",
        '    if [ -f "$file" ]; then',
        '        mv "$file" common_failures/',
        "    else",
        '        echo "File not found: $file"',
        "    fi",
        "done < unique_failures.txt",
        'echo "All files that failed a parameter check have been moved to common_failures/"',
        "",
        "# Move all *Fail folders into common_failures",
        "for fail_dir in *Fail; do",
        '    if [ -d "$fail_dir" ]; then',
        '        mv "$fail_dir" common_failures/',
        "    else",
        '        echo "Directory not found: $fail_dir"',
        "    fi",
        "done",
        "",  # Bug here ??? It's trying to mv '{sorted_}paramList.txt' instead of '{sorted_}newparamList.txt' for some reason...
        "",
        "mv sorted_$paramList common_failures/",
        "mv $paramList common_failures/",
        "mv sorted_paramList.txt common_failures/",
        'echo "All PARAMETERFail, parameter logs, and PARAMETERFail.log{0}s and files have been moved to common_failures/"'.format(
            "'"
        ),
        "",
        'echo "All other parameter (nchan,freq,bw,npol,dm) checks are complete..."',
        'echo "---------------------------------------------"',
        'echo "---------------------------------------------"',
        "",
        'echo "newParamCheck.sh has finished...  .      .        ."',
    ]
    write_script(
        "newParamCheck.sh", lines_newParamCheck, force_overwrite=force_overwrite
    )


def write_processing_creation(force_overwrite=False):
    lines_processing_creation = [
        "#!/bin/bash",
        "",
        "# Generator script that checks that pulsar's data are present and outputs the following files:",
        "## ephemNconvert.txt, clean5G.txt, clean.txt, beamWeight.txt, scrunch.txt - text files with lists of commands parallalized by beam_#",
        "## parallel_${step}.sh - shell scripts to run the above text files using 'parallel' (launch as SLURM job)",
        "",
        "source config.sh",
        "",
        "# Check if there is more than one pulsar data in the directory",
        'pulsar_name=$(check_pulsar_names "$data_directory")',
        "status=$?",
        "",
        "# Check the status and handle errors",
        "if [ $status -ne 0 ]; then",
        "    exit $status",
        "fi",
        "",
        "# If we reach here, it means exactly one pulsar name was found and stored in pulsar_name",
        'echo "Pulsar name found: $pulsar_name"',
        "",
        "# Find par file to install ephemeris",
        'par_file=$(find_par "$pulsar_name")',
        "status=$?",
        "",
        "# Check the status and handle errors",
        "if [ $status -ne 0 ]; then",
        "    exit $status",
        "fi",
        "",
        "# If we reach here, it means the par file was found and stored in par_file",
        'echo "Par file found: $par_file"',
        'echo "---------------------------------------------"',
        "",
        "#--------------------------------------#",
        "#---------------CLEAN RFI--------------#",
        "",
        "# Name the text files to store the commands",
        'ephem_txt="ephemNconvert.txt"',
        'clean5G_txt="clean5G.txt"',
        'clean_txt="clean.txt"',
        "",
        "# Remove pre-existing text files",
        'remove_file_if_exists "$ephem_txt"',
        'remove_file_if_exists "$clean5G_txt"',
        'remove_file_if_exists "$clean_txt"',
        "",
        "# RFI clean each .ar file by beam",
        '# Install ephemeris before averaging to ensure best data quality (Bradley Meyers)" > "$ephem_txt"',
        '# And convert to a psrfits format for compatibility downstream" >> "$ephem_txt"',
        "# Each text file is a list of arguments to run with parallel (don't include comments or blank spaces - everything will be ran as a job)",
        "",
        "# Iterate through each unique beam variation",
        "for beam in $beam_variations; do",
        '    pulsarbeam="${pulsar_name}_${beam}"',
        "    # Install ephemeris before averaging to ensure best data quality (Bradley Meyers)",
        "    # Also update header DMs, if desired",
        '    if [ "$dm" = "ephemeris" ]; then',
        '        echo "for f in \\$(ls CHIME*${beam}*.ar); do pam -p -E ${par_file} --update_dm -a PSRFITS -u . \\$f >> ephemNconvert_${pulsarbeam}-\\${SLURM_JOB_ID}.out 2>>ephemNconvert_${pulsarbeam}-\\${SLURM_JOB_ID}.err; done" >> "$ephem_txt"',
        '    elif [ "$dm" = true ]; then',
        '        echo "for f in \\$(ls CHIME*${beam}*.ar); do pam -p -E ${par_file} -d ${dm} -a PSRFITS -u . \\$f >> ephemNconvert_${pulsarbeam}-\\${SLURM_JOB_ID}.out 2>>ephemNconvert_${pulsarbeam}-\\${SLURM_JOB_ID}.err; done" >> "$ephem_txt"',
        "    else",
        '        echo "for f in \\$(ls CHIME*${beam}*.ar); do pam -p -E ${par_file} -a PSRFITS -u . \\$f >> ephemNconvert_${pulsarbeam}-\\${SLURM_JOB_ID}.out 2>>ephemNconvert_${pulsarbeam}-\\${SLURM_JOB_ID}.err; done" >> "$ephem_txt"',
        "    fi",
        "    # Zap known bad channels (5G zapping from Bradley plus list of commonly bad channels from Emmanuel)",
        '    echo "for f in \\$(ls CHIME*${beam}*.ar); do psrsh chime_zap.psh -e ar.zap \\$f >> clean5G_${pulsarbeam}-\\${SLURM_JOB_ID}.out 2>>clean5G_${pulsarbeam}-\\${SLURM_JOB_ID}.err; done" >> "$clean5G_txt"',
        "    # Run clfd",
        '    echo "for f in \\$(ls CHIME*${beam}*.zap); do clfd \\$f >> clean_${pulsarbeam}-\\${SLURM_JOB_ID}.out 2>>clean_${pulsarbeam}-\\${SLURM_JOB_ID}.err; done" >> "$clean_txt"',
        "done",
        "",
        "# Did it run?",
        'check_file_exists "$ephem_txt"',
        'check_file_exists "$clean5G_txt"',
        'check_file_exists "$clean_txt"',
        "",
        "#--------------------------------------#",
        "#-------------Beam Weight--------------#",
        "",
        "bmWt_txt='beamWeight.txt'",
        'remove_file_if_exists "$bmWt_txt"',
        '# Run beam weighting on each file > "$bmWt_txt"',
        "",
        "# Iterate through each unique beam variation",
        "for beam in $beam_variations; do",
        '    outfile_base="beamWeight_${pulsar_name}_${beam}"',
        "    # Create a script for the specific beam variation",
        '    echo "add_beam -vv -e bmwt CHIME*${beam}*.clfd >>${outfile_base}-\\${SLURM_JOB_ID}.out 2>>${outfile_base}-\\${SLURM_JOB_ID}.err" >> "$bmWt_txt"',
        "done",
        "",
        "# Did it run?",
        'check_file_exists "$bmWt_txt"',
        "",
        "#--------------------------------------#",
        "#---------------Scrunch----------------#",
        "",
        "scrunch_txt='scrunch.txt'",
        'remove_file_if_exists "$scrunch_txt"',
        '# Scrunch files in time and frequency > "$scrunch.txt"',
        "",
        "# Iterate through each unique beam variation",
        "for beam in $beam_variations; do",
        '    outfile_base="scrunch_${pulsar_name}_${beam}"',
        "    # Create a script for the specific beam variation",
        "    # Use nsubbands value from config.sh",
        '    echo "for f in \\$(ls CHIME*${0}*bmwt.clfd); do nsub=\\$(vap -nc length \\$f | awk {1}); pam --setnchn $nsubbands -e ftp --setnsub \\$nsub \\$f >> ${2}-\\${3}.out 2>>${2}-\\${3}.err; done" >> "$scrunch_txt"'.format(
            "{beam}",
            "'{print int(\\$2/$max_subint) + 1}'",
            "{outfile_base}",
            "{SLURM_JOB_ID}",
        ),
        "done",
        "",
        "# Did it run?",
        'check_file_exists "$scrunch_txt"',
        "",
        "#--------------------------------------#",
        "#----------mk parallel_run.sh----------#",
        "",
        "# Define the steps in an array",
        'steps=("ephemNconvert" "clean5G" "clean" "beamWeight" "scrunch")',
        "",
        "# Function to generate the parallel script for each step",
        "generate_parallel_script() {",
        "    local step=$1",
        '    local script_name="parallel_${step}.sh"',
        "",
        '    remove_file_if_exists "$script_name"',
        "    ",
        "    # Write the SLURM header, modules to load, and general parallel command.",
        "    cat <<EOL > $script_name",
        "#!/bin/bash",
        "",
        "#SBATCH --cpus-per-task=$num_beam",
        "#SBATCH --job-name=${step}_${pulsar_name} # Job name",
        "#SBATCH --error=%x-%j.err                 # Error file name = jobname-jobID.out",
        "",
        "# Our par files use the tempo site code for CHIME, 'CH'. So we set:",
        "",
        "export TEMPO2_ALIAS='tempo'",
        "",
        "# Print job ID",
        'echo "Job ID: \\$SLURM_JOB_ID"',
        "",
        "# Run the parallel command for $step",
        "parallel --env _ --jobs \\$SLURM_CPUS_PER_TASK --joblog ${step}_${pulsar_name}.log < ./${step}.txt",
        "",
        "# Join the individual .out/.err files into one",
        "cat ${step}_${pulsar_name}_beam*-\\${SLURM_JOB_ID}.out >> ${step}_${pulsar_name}.out",
        "rm ${step}_${pulsar_name}_beam*-\\${SLURM_JOB_ID}.out",
        "cat ${step}_${pulsar_name}_beam*-\\${SLURM_JOB_ID}.err >> ${step}_${pulsar_name}-\\${SLURM_JOB_ID}.err",
        "rm ${step}_${pulsar_name}_beam*-\\${SLURM_JOB_ID}.err",
        "",
        "EOL",
        "",
        '    check_file_exists "$script_name"',
        "",
        "    # Make the script executable",
        "    chmod +x $script_name",
        "}",
        "",
        "# Loop through the steps array and create a script for each step",
        'for step in "${steps[@]}"; do',
        "    generate_parallel_script $step",
        "done",
    ]
    write_script(
        "processing_creation.sh",
        lines_processing_creation,
        force_overwrite=force_overwrite,
    )


def write_scrunch(force_overwrite=False):
    lines_scrunch = [
        "#!/bin/bash",
        "",
        "#SBATCH --error=%x-%j.err                 # Error file name = jobname-jobID.err",
        "",
        "# Print job ID",
        'echo "Job ID: $SLURM_JOB_ID"',
        "",
        "source config.sh",
        "",
        "# Check if there is more than one pulsar data in the directory",
        'pulsar_name=$(check_pulsar_names "$data_directory")',
        "status=$?",
        "",
        "# Check the status and handle errors",
        "if [ $status -ne 0 ]; then",
        "    exit $status",
        "fi",
        "",
        "# If we reach here, it means exactly one pulsar name was found and stored in pulsar_name",
        'echo "Pulsar name found: $pulsar_name"',
        "",
        "# Scrunch files in time and frequency",
        "# Use nsubbands value from config.sh",
        "for f in $(ls CHIME*bmwt.clfd); do",
        '    nsub=$(vap -nc length $f | awk -v max_subint="$max_subint" {0})'.format(
            "'{print int($2/max_subint) + 1}'"
        ),
        "    pam --setnchn $nsubbands -e ftp --setnsub $nsub $f >> scrunch_${pulsar_name}-${SLURM_JOB_ID}.out 2>>scrunch_${pulsar_name}-${SLURM_JOB_ID}.err",
        "done",
    ]
    write_script("scrunch.sh", lines_scrunch, force_overwrite=force_overwrite)


def write_template_creation(force_overwrite=False):
    lines_template_creation = [
        "#!/bin/bash",
        "",
        "#SBATCH --ntasks=1             # Single Job is created, sets the # of jobs/commands launched",
        "#SBATCH --error=%x-%j.err      # Error file name = jobname-jobID.err",
        "",
        "# Generator file for creating a smoothed template",
        "# Also checks that one pulsar's data is present and if a par file exists in par_directory with the same name",
        "# File created: template_run.sh",
        "",
        "source config.sh",
        "",
        "# Print job ID",
        'echo "Job ID: $SLURM_JOB_ID"',
        "",
        "###########################################################",
        "##                    Gather files                       ##",
        "###########################################################",
        "",
        "# The most recent nbin value is determined in allParamCheck.sh",
        'echo "Gathering 50 highest S/N files with the most recent nbin value (=${template_nbin})..."',
        "",
        "# Use psrstat to get S/N and nbin for each file, sort by S/N descending,",
        "# and only accept files that match the most recent nbin value stored as template_nbin in config.sh",
        'sorted_files=$(for file in "$data_directory/"*${template_ext}; do',
        "                    # Extract frequency-scrunched SNR snr and nbin values",
        '                    data=$(psrstat -c snr,nbin -j DFTp -Q "$file")',
        "                    snr=$(echo $data | awk '{print $2}')",
        "                    nbin=$(echo $data | awk '{print $3}')",
        "",
        "                    # Convert snr and nbin to integers",
        '                    snr=$(printf "%.0f" "$snr")',
        '                    nbin=$(printf "%.0f" "$nbin")',
        "",
        "		    # Compare nbin with template_nbin",
        '                    if [ "$nbin" -eq "$template_nbin" ]; then',
        '                        echo "$file $snr $nbin"',
        "                    fi",
        "               done | sort -k2,2nr)",
        "",
        "",
        "# Check if sorted_files is empty",
        'if [ -z "$sorted_files" ]; then',
        '    echo "No files found with nbin=${template_nbin}. Investigate files. Exiting..."',
        "    exit 1",
        "fi",
        "",
        "# Store all filenames, snr, and nbin values in template_allFiles.txt",
        'echo "$sorted_files" > template_allFiles.txt',
        "",
        "# Select the top 50 files with highest SNR and matching nbin",
        'top_50_files=$(echo "$sorted_files" | head -n 50 | awk {0})'.format(
            "'{print $1}'"
        ),
        "",
        "# Check if top_50_files is empty or has fewer than 50 files",
        'if [ -z "$top_50_files" ] || [ "$(echo "$top_50_files" | wc -l)" -lt 50 ]; then',
        '    echo "Not enough top 50 files found with nbin=${template_nbin}. Investigate template_allFiles.txt. Exiting..."',
        "    exit 1",
        "fi",
        "",
        "# Store the top 50 filenames in template_50.txt",
        'echo "$top_50_files" > template_50.txt',
        "",
        "# Did it run?",
        "check_file_exists template_allFiles.txt",
        "check_file_exists template_50.txt",
        'echo "---------------------------------------------"',
        "",
        "###########################################################",
        "##                Create template commands               ##",
        "###########################################################",
        "",
        "# Check if there is more than one pulsar data in the directory",
        'pulsar_name=$(check_pulsar_names "$data_directory")',
        "status=$?",
        "",
        "# Check the status and handle errors",
        "if [ $status -ne 0 ]; then",
        "    exit $status",
        "fi",
        "",
        "# If we reach here, it means exactly one pulsar name was found and stored in pulsar_name",
        'echo "Pulsar name found: $pulsar_name"',
        "",
        "# Find par file to install ephemeris",
        'par_file=$(find_par "$pulsar_name")',
        "status=$?",
        "",
        "# Check the status and handle errors",
        "if [ $status -ne 0 ]; then",
        "    exit $status",
        "fi",
        "",
        "# If we reach here, it means the par file was found and stored in par_file",
        'echo "Par file found: $par_file"',
        'echo "---------------------------------------------"',
        "",
        "# We will name the template with the pulsar name and today's date",
        'today=$(date +"%Y-%m-%d")',
        'template="${pulsar_name}.Rcvr_CHIME.CHIME.${today}.sum.sm"',
        "",
        "# Check if par file was found and create template_run.sh",
        'if [ -n "$par_file" ]; then',
        '    template_sh="template_run.sh"',
        "",
        "    # Create the template_run.sh file",
        "    {",
        "        echo '#!/bin/bash'",
        "        echo ''",
        "        echo '#SBATCH --cpus-per-task=1'  # One task per CPU",
        '        echo "#SBATCH --job-name=template_run_${pulsar_name}"  # Job name',
        "        echo '#SBATCH --error=%x-%j.err'        # Error file name = jobname-jobID.err",
        "        # echo '' # these lines aren't necessary if the environment is set up correctly",
        "        # echo 'module use /project/6004902/chimepsr-software/v2/environment-modules'",
        "        # echo ''",
        "        # echo 'module load psrchive'",
        "        echo ''",
        '        echo "# Our par files use the tempo site code for CHIME, {0}. So we set:"'.format(
            "'CH'"
        ),
        '        echo "export TEMPO2_ALIAS={0}"'.format("'tempo'"),
        "        echo ''",
        "        echo '# Print job ID.'",
        '        echo "echo Job ID: \\$SLURM_JOB_ID"',
        "        echo ''",
        '        echo "# Inital model = Gaussian w/ width=0.1, max iterations=3, save output to add.trimmed, on *${0}s."'.format(
            "{template_ext}'"
        ),
        "        echo 'autotoa -g0.1 -i3 -S added.trimmed -M template_50.txt'",
        "        echo '# Rotate template so peak is centered.'",
        "        echo 'pam -r0.5 -m added.trimmed'",
        "        echo 'psrsmooth -W added.trimmed'",
        "        echo '# Rename the template to [JB]####+/-####.Rcvr_CHIME.CHIME.YYYY-MM-DD.sum.sm'",
        '        echo "mv added.trimmed.sm ${template}"',
        '    } > "$template_sh"',
        "",
        "    # Check if template_run.sh was created successfully",
        '    check_file_exists "$template_sh"',
        '    echo "---------------------------------------------"',
        "fi",
    ]
    write_script(
        "template_creation.sh", lines_template_creation, force_overwrite=force_overwrite
    )


def write_tim_creation(force_overwrite=False):
    lines_tim_creation = [
        "#!/bin/bash",
        "",
        "# Generator script to create files needed to produce TOA file aka *.tim ",
        "# Also checks that a smoothed profile exists from (psrsmooth) and renames it",
        "# File created: tim_run.sh",
        "",
        "source config.sh",
        "",
        "# Check if there is more than one pulsar data in the directory",
        'pulsar_name=$(check_pulsar_names "$data_directory")',
        "status=$?",
        "",
        "# Check the status and handle errors",
        "if [ $status -ne 0 ]; then",
        "    exit $status",
        "fi",
        "",
        "# If we reach here, it means exactly one pulsar name was found and stored in pulsar_name",
        'echo "Pulsar name found: $pulsar_name"',
        'echo "---------------------------------------------"',
        "",
        "###########################################################",
        "##                     .tim creation                     ##",
        "###########################################################",
        "",
        "# Check that the template specified in config.sh exists",
        'template_path="${data_directory}/${template}"',
        "",
        'if [ -e "$template_path" ]; then',
        '    tim_sh="tim_run.sh" ',
        "",
        '    echo "Template file found: $template"',
        "",
        "    # Create the .tim file name for pat to write to",
        '    today=$(date +"%Y-%m-%d")',
        '    tim="${pulsar_name}.Rcvr_CHIME.CHIME.${today}.nb.tim"',
        '    echo "---------------------------------------------"',
        "",
        "    # Create the tim_run.sh file",
        "    {",
        "        echo '#!/bin/bash'",
        "        echo ''",
        '        echo "#SBATCH --job-name=tim_run_${pulsar_name}"  # Job name',
        "        echo '#SBATCH --error=%x-%j.err'        # Error file name = jobname-jobID.err",
        "        # echo '' # these lines aren't necessary if the environment is set up correctly",
        "        # echo 'module use /project/6004902/chimepsr-software/v2/environment-modules'",
        "        # echo 'module load psrchive'",
        "        echo ''",
        '        echo "# Our par files use the tempo site code for CHIME, {0}. So we set:"'.format(
            "'CH'"
        ),
        '        echo "export TEMPO2_ALIAS={0}"'.format("'tempo'"),
        "        echo ''",
        "        echo '# Print the Job ID'",
        '        echo "echo Job ID: \\$SLURM_JOB_ID"',
        "        echo ''",
        '        echo "pat -A FDM -e mcmc=0 -C chan -C subint -C snr -C wt -f \\"tempo2\\" -X \\"${tim_flags}\\" -s $template *$template_ext > ${tim}"',
        # NB: adding a line containing any characters after the pat command will break the pipeline :) -Will
        '    } > "$tim_sh"',
        "",
        "    # Check if tim_run.sh was created successfully",
        '    check_file_exists "$tim_sh"',
        '    echo "---------------------------------------------"',
        "",
        "else",
        '    echo "Error: smoothed template specified by config.sh (${template}) not found in ${data_directory}."',
        '    echo "---------------------------------------------"',
        "    exit 1",
        "fi",
    ]
    write_script("tim_creation.sh", lines_tim_creation, force_overwrite=force_overwrite)


def write_unpack_tar(force_overwrite=False):
    lines_unpack_tar = [
        "#!/bin/bash",
        "",
        "#SBATCH --error=%x-%j.err     # Error file name = jobname-jobID.err",
        "",
        "# Script to unpack tar files of old CHIME data",
        "# Old chime data on Cedar lives: ~/nearline/rrg-istairs-ad/archive/pulsar/chime/fold_mode/pulsar_name",
        "",
        "# Print job ID",
        'echo "Job ID: $SLURM_JOB_ID"',
        "",
        "for file in *.tar; do",
        '    if [ -f "$file" ]; then  # Check if it is a regular file',
        '        echo "Extracting $file..."',
        '        tar -xf "$file"',
        '        count=$(tar -tf "$file" | wc -l) # list all the files in the tar and count each line',
        '        echo "Extracted $count file(s) from $file"',
        "    fi",
        "done",
    ]
    write_script("unpack_tar.sh", lines_unpack_tar, force_overwrite=force_overwrite)
